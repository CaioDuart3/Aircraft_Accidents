{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6654f0e",
   "metadata": {},
   "source": [
    "# Processamento de Dados: Camada Raw para Silver\n",
    "**Disciplina:** Sistemas de Banco de Dados 2  \n",
    "**Semestre:** 2025/2  \n",
    "**Professor:** Thiago Luiz de Souza Gomes  \n",
    "**Grupo 15**\n",
    "\n",
    "**Integrantes:**\n",
    "* Caio Ferreira Duarte (231026901)\n",
    "* Laryssa Felix Ribeiro Lopes (231026840)\n",
    "* Luísa de Souza Ferreira (232014807)\n",
    "* Henrique Fontenelle Galvão Passos (231030771)\n",
    "* Marjorie Mitzi Cavalcante Rodrigues (231039140)\n",
    "\n",
    "---\n",
    "\n",
    "## Contextualização e Objetivos\n",
    "O objetivo deste script é extrair os dados da One Big Table(`aviao`) localizada no schema `silver` do banco de dados, transformá-los em tabelas de fatos e dimensões por meio da modelagem dimensional no formato star schema e, por fim, carregá-los no schema `dw`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fce59",
   "metadata": {},
   "source": [
    "# 1. Preparação de Bibliotecas e conexões externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5007a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas e conexão configurada!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlparse\n",
    "from sqlalchemy import create_engine, text\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Configurações de conexão\n",
    "DB_URI = \"postgresql://admin:admin@localhost:5432/db_aviao\"\n",
    "ARQUIVO_DDL = '../Data Layer/gold/ddl.sql'\n",
    "engine = create_engine(DB_URI)\n",
    "\n",
    "print(\"Bibliotecas importadas e conexão configurada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346eab3f",
   "metadata": {},
   "source": [
    "# 2. Executa DDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24176a84",
   "metadata": {},
   "source": [
    "O trecho de código abre o arquivo de DDL (ARQUIVO_DDL) em modo leitura com codificação UTF-8 e utiliza o sqlparse para limpar o conteúdo, removendo comentários e deixando o SQL mais organizado; \n",
    "\n",
    "em seguida, a string limpa é dividida em comandos SQL individuais (commands) e, dentro de um contexto transacional (engine.begin()), cada comando é executado no banco usando conn.execute(text(cmd)), garantindo que todas as instruções sejam aplicadas corretamente.\n",
    "\n",
    "Ao final o DDL é executado e as tabelas são criadas no schema dw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "987f3b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDL executado! Tabelas criadas no schema dw\n"
     ]
    }
   ],
   "source": [
    "# Abre o arquivo DDL (arquivo com instruções SQL de criação de tabelas)\n",
    "with open(ARQUIVO_DDL, 'r', encoding='utf-8') as f:\n",
    "    # Lê o conteúdo do arquivo e remove comentários usando sqlparse\n",
    "    sql_ddl_clean = sqlparse.format(f.read(), strip_comments=True)\n",
    "\n",
    "# Divide o SQL em comandos individuais e remove espaços em branco\n",
    "commands = [cmd.strip() for cmd in sqlparse.split(sql_ddl_clean) if cmd.strip()]\n",
    "\n",
    "# Inicia uma transação com o banco de dados\n",
    "with engine.begin() as conn:\n",
    "    # Executa cada comando SQL individualmente\n",
    "    for cmd in commands:\n",
    "        conn.execute(text(cmd))\n",
    "        \n",
    "print(\"DDL executado! Tabelas criadas no schema dw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929b80e",
   "metadata": {},
   "source": [
    "# 2. Extração dos dados da Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e814ec",
   "metadata": {},
   "source": [
    "Utiliza do pandas para extrair os dados da One Big Table da camada Silver e armazena em um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1af02840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87,951 registros extraídos da silver.aviao\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>investigation_type</th>\n",
       "      <th>accident_number</th>\n",
       "      <th>event_date</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>airport_code</th>\n",
       "      <th>...</th>\n",
       "      <th>air_carrier</th>\n",
       "      <th>broad_phase_of_flight</th>\n",
       "      <th>report_status</th>\n",
       "      <th>total_fatal_injuries</th>\n",
       "      <th>total_serious_injuries</th>\n",
       "      <th>total_minor_injuries</th>\n",
       "      <th>total_uninjured</th>\n",
       "      <th>injury_severity</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001218X45444</td>\n",
       "      <td>Accident</td>\n",
       "      <td>SEA87LA080</td>\n",
       "      <td>1948-10-24</td>\n",
       "      <td>None</td>\n",
       "      <td>MOOSE CREEK, ID</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Cruise</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fatal</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2026-01-19 12:35:00.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001218X45447</td>\n",
       "      <td>Accident</td>\n",
       "      <td>LAX94LA336</td>\n",
       "      <td>1962-07-19</td>\n",
       "      <td>1996-09-19</td>\n",
       "      <td>BRIDGEPORT, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fatal</td>\n",
       "      <td>UNK</td>\n",
       "      <td>2026-01-19 12:35:00.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20061025X01555</td>\n",
       "      <td>Accident</td>\n",
       "      <td>NYC07LA005</td>\n",
       "      <td>1974-08-30</td>\n",
       "      <td>2007-02-26</td>\n",
       "      <td>Saltville, VA</td>\n",
       "      <td>United States</td>\n",
       "      <td>36.922223</td>\n",
       "      <td>-81.878056</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Cruise</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fatal</td>\n",
       "      <td>IMC</td>\n",
       "      <td>2026-01-19 12:35:00.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001218X45448</td>\n",
       "      <td>Accident</td>\n",
       "      <td>LAX96LA321</td>\n",
       "      <td>1977-06-19</td>\n",
       "      <td>2000-09-12</td>\n",
       "      <td>EUREKA, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Cruise</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fatal</td>\n",
       "      <td>IMC</td>\n",
       "      <td>2026-01-19 12:35:00.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20041105X01764</td>\n",
       "      <td>Accident</td>\n",
       "      <td>CHI79FA064</td>\n",
       "      <td>1979-08-02</td>\n",
       "      <td>1980-04-16</td>\n",
       "      <td>Canton, OH</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Approach</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fatal</td>\n",
       "      <td>VMC</td>\n",
       "      <td>2026-01-19 12:35:00.806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         event_id investigation_type accident_number  event_date  \\\n",
       "0  20001218X45444           Accident      SEA87LA080  1948-10-24   \n",
       "1  20001218X45447           Accident      LAX94LA336  1962-07-19   \n",
       "2  20061025X01555           Accident      NYC07LA005  1974-08-30   \n",
       "3  20001218X45448           Accident      LAX96LA321  1977-06-19   \n",
       "4  20041105X01764           Accident      CHI79FA064  1979-08-02   \n",
       "\n",
       "  publication_date         location        country   latitude  longitude  \\\n",
       "0             None  MOOSE CREEK, ID  United States        NaN        NaN   \n",
       "1       1996-09-19   BRIDGEPORT, CA  United States        NaN        NaN   \n",
       "2       2007-02-26    Saltville, VA  United States  36.922223 -81.878056   \n",
       "3       2000-09-12       EUREKA, CA  United States        NaN        NaN   \n",
       "4       1980-04-16       Canton, OH  United States        NaN        NaN   \n",
       "\n",
       "  airport_code  ... air_carrier broad_phase_of_flight   report_status  \\\n",
       "0         None  ...        None                Cruise  Probable Cause   \n",
       "1         None  ...        None               Unknown  Probable Cause   \n",
       "2         None  ...        None                Cruise  Probable Cause   \n",
       "3         None  ...        None                Cruise  Probable Cause   \n",
       "4         None  ...        None              Approach  Probable Cause   \n",
       "\n",
       "  total_fatal_injuries total_serious_injuries  total_minor_injuries  \\\n",
       "0                    2                      0                     0   \n",
       "1                    4                      0                     0   \n",
       "2                    3                      0                     0   \n",
       "3                    2                      0                     0   \n",
       "4                    1                      2                     0   \n",
       "\n",
       "   total_uninjured injury_severity weather_condition              created_at  \n",
       "0                0           Fatal               UNK 2026-01-19 12:35:00.806  \n",
       "1                0           Fatal               UNK 2026-01-19 12:35:00.806  \n",
       "2                0           Fatal               IMC 2026-01-19 12:35:00.806  \n",
       "3                0           Fatal               IMC 2026-01-19 12:35:00.806  \n",
       "4                0           Fatal               VMC 2026-01-19 12:35:00.806  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_obt = pd.read_sql(\"SELECT * FROM silver.aviao\", engine)\n",
    "print(f\"{len(df_obt):,} registros extraídos da silver.aviao\")\n",
    "df_obt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023dfb4",
   "metadata": {},
   "source": [
    "# 3. Transformação das Dimensões"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438be46a",
   "metadata": {},
   "source": [
    "Função para organizar as colunas da One Big Table em novas tabelas dimensões.\n",
    "\n",
    "Receberá a One Big Table e um mapeamento de colunas, este mapeamento tem nome das colunas na Silver como chave e nome das colunas no DW como valor.\n",
    "\n",
    "Por fim a função retorna um dataframe da nova tabela\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c3e4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dim(df, mapping):\n",
    "    \"\"\"Filtra colunas da Silver, remove duplicatas e renomeia para o DW.\"\"\"\n",
    "    return df[list(mapping.keys())].drop_duplicates().rename(columns=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc581e9",
   "metadata": {},
   "source": [
    "Agora para cada uma das dimensões(tabelas) que desejamos ter no DW, aplicaremos na função `prep_dim` a df_obt, que é de onde sairão os dados, e o mapeamento das colunas referente á aquela dimensão a ser obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1bcb631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_tim: 49,823 registros únicos\n",
      "dim_sev: 10,667 registros únicos\n",
      "dim_wth: 5 registros únicos\n",
      "dim_flt_phs: 13 registros únicos\n",
      "dim_geo: 60,245 registros únicos\n",
      "dim_arc: 86,058 registros únicos\n",
      "dim_opt: 14,656 registros únicos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Dimensão Temporal\n",
    "dim_time = prep_dim(df_obt, {\n",
    "    # nome da coluna na OBT:  nome da coluna no DW\n",
    "    \"event_date\": \"evt_dat\", \n",
    "    \"publication_date\": \"pub_dat\"\n",
    "})\n",
    "\n",
    "# 2. Dimensão Severidade\n",
    "dim_severity = prep_dim(df_obt, {\n",
    "    \"injury_severity\": \"inj_sev\",\n",
    "    \"investigation_type\": \"inv_typ\",\n",
    "    \"report_status\": \"rpt_sta\"\n",
    "})\n",
    "\n",
    "# 3. Dimensão Clima\n",
    "dim_weather = prep_dim(df_obt, {\n",
    "    \"weather_condition\": \"wth_con\"\n",
    "})\n",
    "\n",
    "# 4. Dimensão Fase de Voo\n",
    "dim_flight_phase = prep_dim(df_obt, {\n",
    "    \"broad_phase_of_flight\": \"brd_phs_off_flt\"\n",
    "})\n",
    "\n",
    "# 5. Dimensão Geográfica\n",
    "dim_geograph = prep_dim(df_obt, {\n",
    "    \"country\": \"ctr\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\", \n",
    "    \"airport_code\": \"apt_cod\",\n",
    "    \"airport_name\": \"apt_nam\",\n",
    "    \"location\": \"loc\"\n",
    "})\n",
    "\n",
    "# 6. Dimensão Aeronave\n",
    "dim_aircraft = prep_dim(df_obt, {\n",
    "    \"aircraft_category\": \"arc_cat\", \n",
    "    \"make\": \"mak\", \n",
    "    \"model\": \"mod\", \n",
    "    \"registration_number\": \"reg_num\", \n",
    "    \"engine_type\": \"eng_typ\", \n",
    "    \"number_of_engines\": \"num_off_eng\", \n",
    "    \"amateur_built\": \"ama_blt\", \n",
    "    \"aircraft_damage\": \"arc_dam\"\n",
    "})\n",
    "\n",
    "# 7. Dimensão Operação\n",
    "dim_operation = prep_dim(df_obt, {\n",
    "    \"purpose_of_flight\": \"prp_off_flt\", \n",
    "    \"schedule\": \"sch\", \n",
    "    \"air_carrier\": \"air_car\", \n",
    "    \"far_description\": \"far_dsc\"\n",
    "})\n",
    "\n",
    "print(f\"dim_tim: {len(dim_time):,} registros únicos\")\n",
    "print(f\"dim_sev: {len(dim_severity):,} registros únicos\")\n",
    "print(f\"dim_wth: {len(dim_weather):,} registros únicos\")\n",
    "print(f\"dim_flt_phs: {len(dim_flight_phase):,} registros únicos\")\n",
    "print(f\"dim_geo: {len(dim_geograph):,} registros únicos\")\n",
    "print(f\"dim_arc: {len(dim_aircraft):,} registros únicos\")\n",
    "print(f\"dim_opt: {len(dim_operation):,} registros únicos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f56b6e3",
   "metadata": {},
   "source": [
    "# 4. Carregamento das Dimensões\n",
    "O trecho de código a seguir realiza a carga dos DataFrames das dimensões para o banco de dados utilizando o método to_sql do pandas.\n",
    "\n",
    "Onde cada DataFrame é inserido na tabela correspondente dentro do esquema dw por meio da conexão engine; o parâmetro if_exists='append' garante que, se a tabela já existir, os novos registros sejam adicionados sem apagar os dados anteriores, isso vale principalmente para não apagar as colunas sem dados, e index=False evita que o índice do DataFrame vire uma coluna no banco, mantendo apenas os dados relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da69774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TODAS AS DIMENSÕES CARREGADAS NO BANCO!\n"
     ]
    }
   ],
   "source": [
    "dim_time.to_sql('dim_tim', engine, schema='dw', if_exists='append', index=False)\n",
    "dim_severity.to_sql('dim_sev', engine, schema='dw', if_exists='append', index=False)\n",
    "dim_weather.to_sql('dim_wth', engine, schema='dw', if_exists='append', index=False)\n",
    "dim_flight_phase.to_sql('dim_flt_phs', engine, schema='dw', if_exists='append', index=False)\n",
    "dim_geograph.to_sql('dim_geo', engine, schema='dw', if_exists='append', index=False)\n",
    "dim_aircraft.to_sql('dim_arc', engine, schema='dw', if_exists='append', index=False)\n",
    "dim_operation.to_sql('dim_opt', engine, schema='dw', if_exists='append', index=False)\n",
    "\n",
    "print(\"\\nTODAS AS DIMENSÕES CARREGADAS NO BANCO!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a6d8f",
   "metadata": {},
   "source": [
    "# 5. Transformação da Fato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f128d",
   "metadata": {},
   "source": [
    "O código a seguir faz leitura das tabelas dimensionais já carregadas no banco de dados com os dados já preenchdios e com SRKs, e armazena em váriaveis do tipo dataframe, assim permitindo que os dados posteriormente sejam utilizados para encontrar as Foreign Keys da tabela fato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "071f520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões lidas do banco com surrogate keys!\n"
     ]
    }
   ],
   "source": [
    "dim_time_db = pd.read_sql(\"SELECT * FROM dw.dim_tim\", engine)\n",
    "dim_severity_db = pd.read_sql(\"SELECT * FROM dw.dim_sev\", engine)\n",
    "dim_weather_db = pd.read_sql(\"SELECT * FROM dw.dim_wth\", engine)\n",
    "dim_flight_phase_db = pd.read_sql(\"SELECT * FROM dw.dim_flt_phs\", engine)\n",
    "dim_geograph_db = pd.read_sql(\"SELECT * FROM dw.dim_geo\", engine)\n",
    "dim_aircraft_db = pd.read_sql(\"SELECT * FROM dw.dim_arc\", engine)\n",
    "dim_operation_db = pd.read_sql(\"SELECT * FROM dw.dim_opt\", engine)\n",
    "\n",
    "print(\"Dimensões lidas do banco com surrogate keys!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019205e",
   "metadata": {},
   "source": [
    "Na célula abaixo, copiamos o dataframe da One Big Table e renomeamos as colunas necessárias para realizar os merges com as dimensões.\n",
    "O objetivo é obter as SRKs (surrogate keys) de cada tabela dimensional, que serão utilizadas como FKs na tabela fato.\n",
    "O resultado é armazenado em df_fact_prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebb87aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe preparado para merges!\n"
     ]
    }
   ],
   "source": [
    "# Preparar df_obt para os merges (renomear colunas para mnemônicos)\n",
    "df_fact_prep = df_obt.copy()\n",
    "\n",
    "# Renomear colunas para facilitar os merges\n",
    "df_fact_prep = df_fact_prep.rename(columns={\n",
    "    \"event_date\": \"evt_dat\",\n",
    "    \"publication_date\": \"pub_dat\",\n",
    "    \"injury_severity\": \"inj_sev\",\n",
    "    \"investigation_type\": \"inv_typ\",\n",
    "    \"report_status\": \"rpt_sta\",\n",
    "    \"weather_condition\": \"wth_con\",\n",
    "    \"broad_phase_of_flight\": \"brd_phs_off_flt\",\n",
    "    \"country\": \"ctr\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\",\n",
    "    \"airport_code\": \"apt_cod\",\n",
    "    \"airport_name\": \"apt_nam\",\n",
    "    \"location\": \"loc\",\n",
    "    \"aircraft_category\": \"arc_cat\",\n",
    "    \"make\": \"mak\",\n",
    "    \"model\": \"mod\",\n",
    "    \"registration_number\": \"reg_num\",\n",
    "    \"engine_type\": \"eng_typ\",\n",
    "    \"number_of_engines\": \"num_off_eng\",\n",
    "    \"amateur_built\": \"ama_blt\",\n",
    "    \"aircraft_damage\": \"arc_dam\",\n",
    "    \"purpose_of_flight\": \"prp_off_flt\",\n",
    "    \"schedule\": \"sch\",\n",
    "    \"air_carrier\": \"air_car\",\n",
    "    \"far_description\": \"far_dsc\"\n",
    "})\n",
    "\n",
    "print(\"Dataframe preparado para merges!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25975856",
   "metadata": {},
   "source": [
    "O código a seguir realiza uma série de `merge` (junções) entre o DataFrame de fatos (df_fact_prep) e as tabelas dimensionais já carregadas no banco, \n",
    "com o objetivo de trazer as surrogate keys de cada dimensão para o fato e se tornarem as FK da fato.\n",
    "\n",
    "Em cada etapa, o merge é feito usando as colunas de ligação (por exemplo, evt_dat e pub_dat para a dimensão de tempo, mais a Primary Key da dimensão correspondente) isso vai no primeiro parametro.\n",
    "No `on`, colocamos as chaves que farão o merge entre as duas tabelas,\n",
    "O `how='left'` (LEFT JOIN) preserva todos os registros do fato, mesmo que não haja correspondência em alguma dimensão. Ao final, o DataFrame df_fact contém todas as chaves das dimensões (como dim_time_srk, dim_severity_srk, etc.), que são essenciais para montar a tabela fato no modelo dimensional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d7d5a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merges completados! Surrogate keys obtidos!\n"
     ]
    }
   ],
   "source": [
    "# Merge com dim_tim\n",
    "df_fact = df_fact_prep.merge(\n",
    "    dim_time_db[['srk_tim', 'evt_dat', 'pub_dat']],\n",
    "    on=['evt_dat', 'pub_dat'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge com dim_sev\n",
    "df_fact = df_fact.merge(\n",
    "    dim_severity_db[['srk_sev', 'inj_sev', 'inv_typ', 'rpt_sta']],\n",
    "    on=['inj_sev', 'inv_typ', 'rpt_sta'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge com dim_wth\n",
    "df_fact = df_fact.merge(\n",
    "    dim_weather_db[['srk_wth', 'wth_con']],\n",
    "    on=['wth_con'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge com dim_flt_phs\n",
    "df_fact = df_fact.merge(\n",
    "    dim_flight_phase_db[['srk_flt_phs', 'brd_phs_off_flt']],\n",
    "    on=['brd_phs_off_flt'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge com dim_geo\n",
    "df_fact = df_fact.merge(\n",
    "    dim_geograph_db[['srk_geo', 'ctr', 'lat', 'lon', 'apt_cod', 'apt_nam', 'loc']],\n",
    "    on=['ctr', 'lat', 'lon', 'apt_cod', 'apt_nam', 'loc'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge com dim_arc\n",
    "df_fact = df_fact.merge(\n",
    "    dim_aircraft_db[['srk_arc', 'arc_cat', 'mak', 'mod', 'reg_num', 'eng_typ', 'num_off_eng', 'ama_blt', 'arc_dam']],\n",
    "    on=['arc_cat', 'mak', 'mod', 'reg_num', 'eng_typ', 'num_off_eng', 'ama_blt', 'arc_dam'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge com dim_opt\n",
    "df_fact = df_fact.merge(\n",
    "    dim_operation_db[['srk_opt', 'prp_off_flt', 'sch', 'air_car', 'far_dsc']],\n",
    "    on=['prp_off_flt', 'sch', 'air_car', 'far_dsc'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Merges completados! Surrogate keys obtidos!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91d1a4",
   "metadata": {},
   "source": [
    "Cria a tabela fato, após obtidos os SRK (surrogate keys) na célula anterior, fazemos o filtro no dataframe que é resultado de muitos merges para manter somente as colunas que nos interessam para a tabela fato e por fim armazenamos em `fat_acident`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a30f4e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela fato preparada!\n"
     ]
    }
   ],
   "source": [
    "fat_acident = df_fact[[\n",
    "    'srk_tim',\n",
    "    'srk_sev',\n",
    "    'srk_arc',\n",
    "    'srk_geo',\n",
    "    'srk_flt_phs',\n",
    "    'srk_opt',\n",
    "    'srk_wth',\n",
    "    'event_id',\n",
    "    'accident_number',\n",
    "    'total_fatal_injuries',\n",
    "    'total_serious_injuries',\n",
    "    'total_minor_injuries',\n",
    "    'total_uninjured'\n",
    "]].rename(columns={\n",
    "    'event_id': 'evt_ide',\n",
    "    'accident_number': 'acc_num',\n",
    "    'total_fatal_injuries': 'tot_fat_inj',\n",
    "    'total_serious_injuries': 'tot_ser_inj',\n",
    "    'total_minor_injuries': 'tot_min_inj',\n",
    "    'total_uninjured': 'tot_uni'\n",
    "})\n",
    "\n",
    "print(\"Tabela fato preparada!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ede35c",
   "metadata": {},
   "source": [
    "# 6. Carregamento da Fato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286efdc6",
   "metadata": {},
   "source": [
    "Carrega a tabela fato no banco com as devidas SRK passando:\n",
    "- o dataframe que possue somente as colunas da fato;\n",
    "- o schema na qual queremos adicioná-lo\n",
    "- if_exists='append' para não substituir os dados obtidos anteriormente\n",
    "- index=false para não usar os index do dataframe no banco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c890237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABELA FATO CARREGADA NO BANCO!\n"
     ]
    }
   ],
   "source": [
    "fat_acident.to_sql('fat_acc', engine, schema='dw', if_exists='append', index=False)\n",
    "\n",
    "print(\"TABELA FATO CARREGADA NO BANCO!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
